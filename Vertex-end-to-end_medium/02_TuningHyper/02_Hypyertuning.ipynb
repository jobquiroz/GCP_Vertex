{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10cf5ac-57c5-4f47-96d3-7431c32df977",
   "metadata": {},
   "source": [
    "Model with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0e0d8a-ae9f-47a3-b395-e3624124ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS]\n",
      "                             [--dropout_rate DROPOUT_RATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jupyter/.local/share/jupyter/runtime/kernel-a73cb72b-c867-41ff-af29-7aa61a15e4f8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, losses\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=5)\n",
    "parser.add_argument('--dropout_rate', dest='dropout_rate', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(args.dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aa259-c251-4e77-a94a-bfc3101413a0",
   "metadata": {},
   "source": [
    "We’ve already parameterized the dropout rate. Now we need to update the model to expose metrics. The metric we use is validation accuracy, calculated on the validation dataset at the end of each epoch. We’ll implement the metrics reporting using a custom Tensorflow callback, which calls the Hypertune Python library to report the metrics. The Hypertune library essentially just dumps the metrics in some structured format to a temporary folder on the host machine, which will be picked up by the Hypertune service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbf6cb-71e7-42db-9c7e-0d41f12c614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='val_accuracy',\n",
    "          metric_value=logs['val_accuracy'],\n",
    "          global_step=epoch\n",
    "        )\n",
    "        \n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[custom_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900de8d5-afce-4c3a-a9dd-fd7fc429a351",
   "metadata": {},
   "source": [
    "### Reestructure code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995e4ecb-a074-48ee-a9ca-8666237572d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "import argparse\n",
    "import hypertune\n",
    "\n",
    "GCS_PATH_FOR_DATA = 'gs://ma-mx-presales-lab-bucket/vertex-end-to-end/'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=5)\n",
    "parser.add_argument('--dropout_rate', dest='dropout_rate', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def extract(example):\n",
    "    data = tf.io.parse_example(\n",
    "        example,\n",
    "        # Schema of the example.\n",
    "        {\n",
    "          'image': tf.io.FixedLenFeature(shape=(32, 32, 3), dtype=tf.float32),\n",
    "          'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        }\n",
    "        )\n",
    "    return data['image'], data['label']\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset([GCS_PATH_FOR_DATA + filename]).map(extract, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(1024).batch(128).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(args.dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('train.tfrecord')\n",
    "val_dataset = get_dataset('val.tfrecord')\n",
    "test_dataset = get_dataset('test.tfrecord')\n",
    "\n",
    "\n",
    "# A distributed strategy to take advantage of available hardward.\n",
    "# No-op otherwise.\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = create_model()\n",
    "    # Restore from the latest checkpoint if available.\n",
    "    latest_ckpt = tf.train.latest_checkpoint('gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints/')\n",
    "    if latest_ckpt:\n",
    "        model.load_weights(latest_ckpt)\n",
    "\n",
    "# Create a callback to store a check at the end of each epoch.\n",
    "#ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath='gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints/', #+ 'val/',\n",
    "#    monitor='val_loss',\n",
    "#    save_weights_only=True\n",
    "#    )\n",
    "\n",
    "#model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[ckpt_callback])\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='val_accuracy',\n",
    "          metric_value=logs['val_accuracy'],\n",
    "          global_step=epoch\n",
    "        )\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[custom_callback])\n",
    "\n",
    "model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "# Export the model to GCS.\n",
    "model.save(\"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/models/\") + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861ddb3-0ab5-41f7-91e1-116c1ca9cae4",
   "metadata": {},
   "source": [
    "Los siguientes comandos se deben correr desde la terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f7a135-c4fc-49f6-8fd5-6b3b957b87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv task.py bundle/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e387857-ffad-492f-90ec-05707ef2d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python bundle/setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24507926-9b29-4261-8065-99a9dd601e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  471.0 B/  471.0 B]                                                \n",
      "Operation completed over 1 objects/471.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/trainer-0.1.tar.gz gs://ma-mx-presales-lab-bucket/vertex-end-to-end/python_code2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36236bac-532d-4b09-b764-0424fc39f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [6310487558434324480] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud beta ai hp-tuning-jobs describe 6310487558434324480 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai hp-tuning-jobs create --display-name=e2e-tutorial-hpt --region=us-central1 --config=bundle/hpt.yaml --max-trial-count=10 --parallel-trial-count=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf639dd0-e1bf-4760-aa34-65b0a9a03791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-02-17T21:52:59.717750Z'\n",
      "displayName: e2e-tutorial-hpt\n",
      "maxTrialCount: 10\n",
      "name: projects/825075454589/locations/us-central1/hyperparameterTuningJobs/6310487558434324480\n",
      "parallelTrialCount: 2\n",
      "startTime: '2022-02-17T21:53:04Z'\n",
      "state: JOB_STATE_RUNNING\n",
      "studySpec:\n",
      "  metrics:\n",
      "  - goal: MAXIMIZE\n",
      "    metricId: val_accuracy\n",
      "  parameters:\n",
      "  - doubleValueSpec:\n",
      "      maxValue: 0.9\n",
      "      minValue: 0.01\n",
      "    parameterId: dropout_rate\n",
      "trialJobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      acceleratorCount: 2\n",
      "      acceleratorType: NVIDIA_TESLA_V100\n",
      "      machineType: n1-standard-4\n",
      "    pythonPackageSpec:\n",
      "      args:\n",
      "      - --epochs=50\n",
      "      executorImageUri: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-3:latest\n",
      "      packageUris:\n",
      "      - gs://ma-mx-presales-lab-bucket/vertex-end-to-end/python_code2/trainer-0.1.tar.gz\n",
      "      pythonModule: trainer.task\n",
      "    replicaCount: '1'\n",
      "trials:\n",
      "- id: '1'\n",
      "  parameters:\n",
      "  - parameterId: dropout_rate\n",
      "    value: 0.455\n",
      "  startTime: '2022-02-17T21:53:08.983960220Z'\n",
      "  state: REQUESTED\n",
      "- id: '2'\n",
      "  parameters:\n",
      "  - parameterId: dropout_rate\n",
      "    value: 0.250313\n",
      "  startTime: '2022-02-17T21:53:08.984080477Z'\n",
      "  state: ACTIVE\n",
      "updateTime: '2022-02-17T22:01:03.760875Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai hp-tuning-jobs describe 6310487558434324480 --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bec592-846c-4072-a031-32ecd4762867",
   "metadata": {},
   "source": [
    "***\n",
    "**TensorBoard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7e2965f-f2e6-46fa-bc42-90d894b980bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "GCS_PATH_FOR_DATA = 'gs://ma-mx-presales-lab-bucket/vertex-end-to-end/'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=5)\n",
    "parser.add_argument('--dropout_rate', dest='dropout_rate', type=float, default=0.7272)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def extract(example):\n",
    "    data = tf.io.parse_example(\n",
    "        example,\n",
    "        # Schema of the example.\n",
    "        {\n",
    "          'image': tf.io.FixedLenFeature(shape=(32, 32, 3), dtype=tf.float32),\n",
    "          'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        }\n",
    "        )\n",
    "    return data['image'], data['label']\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset([GCS_PATH_FOR_DATA + filename]).map(extract, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(1024).batch(128).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(args.dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('train.tfrecord')\n",
    "val_dataset = get_dataset('val.tfrecord')\n",
    "test_dataset = get_dataset('test.tfrecord')\n",
    "\n",
    "\n",
    "# A distributed strategy to take advantage of available hardward.\n",
    "# No-op otherwise.\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = create_model()\n",
    "    # Restore from the latest checkpoint if available.\n",
    "    latest_ckpt = tf.train.latest_checkpoint('gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints')\n",
    "    if latest_ckpt:\n",
    "        model.load_weights(latest_ckpt)\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S/\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[tensorboard_callback])\n",
    "\n",
    "model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "# Export the model to GCS.\n",
    "model.save(\"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/models/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S/\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a7ad3c-1644-44be-8c89-d5e4b1957333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "ERROR:tensorflow:Couldn't match files for checkpoint gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 23:51:49.995855: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "    313/Unknown - 34s 98ms/step - loss: 1.7553 - accuracy: 0.3611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 23:52:24.844024: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 40s 117ms/step - loss: 1.7553 - accuracy: 0.3611 - val_loss: 1.5511 - val_accuracy: 0.4481\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4035 - accuracy: 0.4927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 23:52:56.195759: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 29s 94ms/step - loss: 1.4035 - accuracy: 0.4927 - val_loss: 1.3456 - val_accuracy: 0.5221\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2624 - accuracy: 0.5479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 23:53:26.788416: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 33s 106ms/step - loss: 1.2624 - accuracy: 0.5479 - val_loss: 1.2469 - val_accuracy: 0.5617\n",
      "79/79 - 3s - loss: 1.2353 - accuracy: 0.5620 - 3s/epoch - 44ms/step\n",
      "INFO:tensorflow:Assets written to: gs://ma-mx-presales-lab-bucket/vertex-end-to-end/models/20220217-235336/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "import datetime\n",
    "\n",
    "GCS_PATH_FOR_DATA = 'gs://ma-mx-presales-lab-bucket/vertex-end-to-end/'\n",
    "\n",
    "def extract(example):\n",
    "    data = tf.io.parse_example(\n",
    "        example,\n",
    "        # Schema of the example.\n",
    "        {\n",
    "          'image': tf.io.FixedLenFeature(shape=(32, 32, 3), dtype=tf.float32),\n",
    "          'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        }\n",
    "        )\n",
    "    return data['image'], data['label']\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset([GCS_PATH_FOR_DATA + filename]).map(extract, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(1024).batch(128).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.0727),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('train.tfrecord')\n",
    "val_dataset = get_dataset('val.tfrecord')\n",
    "test_dataset = get_dataset('test.tfrecord')\n",
    "\n",
    "\n",
    "# A distributed strategy to take advantage of available hardward.\n",
    "# No-op otherwise.\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = create_model()\n",
    "    # Restore from the latest checkpoint if available.\n",
    "    latest_ckpt = tf.train.latest_checkpoint('gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints')\n",
    "    if latest_ckpt:\n",
    "        model.load_weights(latest_ckpt)\n",
    "\n",
    "log_dir = \"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S/\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset, callbacks=[tensorboard_callback])\n",
    "\n",
    "model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "# Export the model to GCS.\n",
    "model.save(\"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/models/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S/\") )"
   ]
  },
  {
   "attachments": {
    "43a7fb6c-4395-4554-a215-34eff502feda.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e11c1c2d-c824-4d57-be05-597dd08658b2",
   "metadata": {},
   "source": [
    "Estructurar como:\n",
    "\n",
    "![image.png](attachment:43a7fb6c-4395-4554-a215-34eff502feda.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c09b18-974c-41e7-b5aa-d695ccb7cb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b69cbd2c-485a-49d6-91a9-714b63f4b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python bundle2/setup.py sdist --format=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6a01357-31d8-4a6b-8603-0dbe296aefdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://bundle2/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  473.0 B/  473.0 B]                                                \n",
      "Operation completed over 1 objects/473.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp bundle2/dist/trainer-0.1.tar.gz gs://ma-mx-presales-lab-bucket/vertex-end-to-end/python_code3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72145153-7827-4051-89f9-25cd2b467cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/825075454589/locations/us-central1/customJobs/4670051394164621312] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud beta ai custom-jobs describe projects/825075454589/locations/us-central1/customJobs/4670051394164621312\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud beta ai custom-jobs stream-logs projects/825075454589/locations/us-central1/customJobs/4670051394164621312\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai custom-jobs create --region=us-central1 --display-name=e2e-tutorial-dropout --config=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45284766-9b3d-4781-bc82-86b99d0233d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tensorboard\n",
    "\n",
    "**Local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a2f71c-f57f-4875-96af-434e7587af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir gs://ma-mx-presales-lab-bucket/vertex-end-to-end/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b4a5c-42b8-4594-b81c-b9122ebf0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/WhBdY1FvTwqYN0MiQF1IzA/\n",
      "\n",
      "\u001b[1m[2022-02-18T00:26:01]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2022-02-18T00:26:02]\u001b[0m Total uploaded: 18 scalars, 30 tensors (21.5 kB), 1 binary objects (60.8 kB)\n",
      "\u001b[1m[2022-02-18T00:26:02]\u001b[0m Done scanning logdir.\n",
      "\n",
      "\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/WhBdY1FvTwqYN0MiQF1IzA/\n"
     ]
    }
   ],
   "source": [
    "!tensorboard dev upload --logdir gs://ma-mx-presales-lab-bucket/vertex-end-to-end/logs/ --name \"Simple experiment\" --description \"Training results from \" --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3161f-587d-4722-9a01-e8941d39447e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
