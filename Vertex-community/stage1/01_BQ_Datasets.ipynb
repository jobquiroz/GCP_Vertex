{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management: get started with BigQuery datasets\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "    <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with BigQuery datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). In this version of the dataset you consider the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_bq"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `BigQuery` as a dataset for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Datasets`\n",
    "- `BigQuery Datasets`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex AI `Dataset` resource from `BigQuery` table -- compatible for `AutoML` training.\n",
    "- Extract a copy of the dataset from `BigQuery` to a CSV file in Cloud Storage -- compatible for `AutoML` or custom training.\n",
    "- Select rows from a `BigQuery` dataset into a `pandas` dataframe -- compatible for custom training.\n",
    "- Select rows from a `BigQuery` dataset into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
    "- Select rows from extracted CSV files into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
    "- Create a `BigQuery` dataset from CSV files.\n",
    "- Extract data from `BigQuery` table into a `DMatrix` -- compatible for custom training `XGBoost` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,tabular,bq"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, following are the best practices when dealing with structured (tabular) data in BigQuery:\n",
    "\n",
    "- For AutoML training:\n",
    "  - Create a managed dataset with Vertex AI `TabularDataset`.\n",
    "  - Use the BigQuery table as the input to the dataset.\n",
    "  - Specify columns and columns transformations when running the AutoML training pipeline job.\n",
    "\n",
    "\n",
    "- For custom training:\n",
    "  - For small datasets:\n",
    "    - Extract the BigQuery to a pandas dataframe.\n",
    "    - Preprocess the data in the dataframe.\n",
    "  - For large datasets:\n",
    "    - TensorFlow model training:\n",
    "      - Create a tf.data.Dataset generator from the BigQuery table.\n",
    "      - Specify the columns for the custrom training.\n",
    "      - Preprocess the data either:\n",
    "        - Within the generator (upstream)\n",
    "        - Within the model (downstream)\n",
    "    - XGBoost model training:\n",
    "      - Use BigQuery ML built-in XGBoost training.\n",
    "      - Alternatively, create a DMatrix generator from CSV files extracted from BigQuery table.\n",
    "    - Pytorch model training:\n",
    "        - Extract the BigQuery to a pandas dataframe.\n",
    "        - Preprocess the data in the dataframe.\n",
    "        - Create a DataLoader generator from the pandas dataframe.\n",
    "\n",
    "\n",
    "- Alternatively:\n",
    "    - Extract the BigQuery table to CSV files.\n",
    "    - Preprocess the CSV files.\n",
    "    - Create a tf.data.Dataset generator from the CSV files.\n",
    "    \n",
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install the following packages to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.0-py3-none-manylinux2014_x86_64.whl (193.7 MB)\n",
      "     |████████████████████████████████| 193.7 MB 25 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U xgboost --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: ma-mx-presales-lab\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]\n",
    "print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### Create a Cloud Storage bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "# Already created\n",
    "\n",
    "BUCKET_URI = \"gs://vertex-ai-mlops-bucket\" #[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "#! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "#! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now, set the variable `IMPORT_FILE` to the location of the data table in BigQuery and `BQ_TABLE` with the table id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "import_file:gsod,bq,lrg"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg"
   },
   "source": [
    "### 1 Create BigQuery Dataset for AutoML\n",
    "\n",
    "#### BigQuery input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
    "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
    "\n",
    "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/825075454589/locations/us-central1/datasets/2682575275308351488/operations/8546273603736305664\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/825075454589/locations/us-central1/datasets/2682575275308351488\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/825075454589/locations/us-central1/datasets/2682575275308351488')\n",
      "projects/825075454589/locations/us-central1/datasets/2682575275308351488\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[IMPORT_FILE],\n",
    "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
    ")\n",
    "\n",
    "label_column = \"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_extract"
   },
   "source": [
    "### 2 Copyign a BigQuery table to a CSV file to Cloud Storage\n",
    "\n",
    "Next, you make a copy of the BigQuery table as a CSV file, to Cloud Storage using the BigQuery extract command.\n",
    "\n",
    "Learn more about [BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bq_extract"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r276809301982e5e1_0000018068983d0f_1 ... (48s) Current status: DONE   \n",
      "['gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000105.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000106.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000107.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000108.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000109.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000110.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000111.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000112.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000113.csv', 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000114.csv']\n",
      "station_number,wban_number,year,month,day,mean_temp,num_mean_temp_samples,mean_dew_point,num_mean_dew_point_samples,mean_sealevel_pressure,num_mean_sealevel_pressure_samples,mean_station_pressure,num_mean_station_pressure_samples,mean_visibility,num_mean_visibility_samples,mean_wind_speed,num_mean_wind_speed_samples,max_sustained_wind_speed,max_gust_wind_speed,max_temperature,max_temperature_explicit,min_temperature,min_temperature_explicit,total_precipitation,snow_depth,fog,rain,snow,hail,thunder,tornado\n",
      "39730,99999,1929,10,20,52.799999237060547,4,45.5,4,,,,,6.1999998092651367,4,21.200000762939453,4,29.899999618530273,,50,false,,,0,,false,false,false,false,false,false\n",
      "33110,99999,1929,12,18,47.5,4,44,4,,,,,2.5,4,11,4,13,,45,false,,,,,false,false,false,false,false,false\n",
      "37770,99999,1931,4,24,50.200000762939453,4,44.299999237060547,4,,,,,5.9000000953674316,4,12.300000190734863,4,18.100000381469727,,45,false,,,,,false,false,false,false,false,false\n",
      "726810,24131,1931,6,23,65.0999984741211,24,41.5,8,,,,,48.299999237060547,24,7.1999998092651367,24,11.100000381469727,,53.400001525878906,true,,,0,,false,false,false,false,false,false\n",
      "726810,24131,1931,3,2,42.799999237060547,24,31.5,8,,,,,72.9000015258789,24,2.2999999523162842,24,4.0999999046325684,,32.400001525878906,true,,,0,,false,false,false,false,false,false\n",
      "726810,24131,1931,9,17,67,24,40.5,8,,,,,33.799999237060547,24,2.4000000953674316,24,6,,51.299999237060547,true,,,0,,false,false,false,false,false,false\n",
      "726810,24131,1931,8,7,68.4000015258789,24,37.200000762939453,8,,,,,27.899999618530273,24,3.5,24,7,,52.299999237060547,true,,,0,,false,false,false,false,false,false\n",
      "726810,24131,1932,7,14,64.0999984741211,24,54.099998474121094,8,,,,,41,24,4.1999998092651367,24,8.8999996185302734,,55.400001525878906,true,,,,,false,false,false,false,false,false\n",
      "726810,24131,1932,10,23,41.099998474121094,24,31,8,,,,,41,24,4.3000001907348633,24,15.899999618530273,,35.400001525878906,true,,,,,false,false,false,false,false,false\n"
     ]
    }
   ],
   "source": [
    "comps = BQ_TABLE.split(\".\")\n",
    "BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n",
    "\n",
    "! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_URI/BQ_2_CSVs/mydata*.csv\n",
    "\n",
    "IMPORT_FILES = ! gsutil ls $BUCKET_URI/BQ_2_CSVs/mydata*.csv\n",
    "\n",
    "print(IMPORT_FILES[-10:])\n",
    "\n",
    "EXAMPLE_FILE = IMPORT_FILES[0]\n",
    "\n",
    "! gsutil cat $EXAMPLE_FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,lrg"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "#### CSV input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
    "\n",
    "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000000.csv',\n",
       " 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000001.csv',\n",
       " 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000002.csv',\n",
       " 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000003.csv',\n",
       " 'gs://vertex-ai-mlops-bucket/BQ_2_CSVs/mydata000000000004.csv']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMPORT_FILES[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "create_dataset:tabular,lrg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/825075454589/locations/us-central1/datasets/9158751539467124736/operations/6591711365457510400\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/825075454589/locations/us-central1/datasets/9158751539467124736\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/825075454589/locations/us-central1/datasets/9158751539467124736')\n",
      "projects/825075454589/locations/us-central1/datasets/9158751539467124736\n"
     ]
    }
   ],
   "source": [
    "gcs_source = IMPORT_FILES\n",
    "\n",
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=gcs_source,\n",
    "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
    ")\n",
    "\n",
    "label_column = \"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_view"
   },
   "source": [
    "### Create a view of the BigQuery dataset\n",
    "\n",
    "Alternatively, you can create a logical view of a BigQuery dataset that has a subset of the fields.\n",
    "\n",
    "Learn more about [Creating BigQuery views](https://cloud.google.com/bigquery/docs/views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7dc142433e50"
   },
   "outputs": [],
   "source": [
    "# Set dataset name and view name in BigQuery\n",
    "BQ_MY_DATASET = \"[your-dataset-name]\"\n",
    "BQ_MY_TABLE = \"[your-view-name]\"\n",
    "\n",
    "# Otherwise, use the default names\n",
    "if (\n",
    "    BQ_MY_DATASET == \"\"\n",
    "    or BQ_MY_DATASET is None\n",
    "    or BQ_MY_DATASET == \"[your-dataset-name]\"\n",
    "):\n",
    "    BQ_MY_DATASET = \"mlops_dataset_\" + TIMESTAMP\n",
    "\n",
    "if BQ_MY_TABLE == \"\" or BQ_MY_TABLE is None or BQ_MY_TABLE == \"[your-view-name]\":\n",
    "    BQ_MY_TABLE = \"mlops_view_\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bq_view"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'ma-mx-presales-lab:mlops_dataset_20220427005512' successfully created.\n",
      "\n",
      "CREATE OR REPLACE VIEW `ma-mx-presales-lab.mlops_dataset_20220427005512.mlops_view_20220427005512`\n",
      "AS SELECT station_number,year,month,day,mean_temp FROM `bigquery-public-data.samples.gsod`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the resources\n",
    "! bq --location=US mk -d \\\n",
    "$PROJECT_ID:$BQ_MY_DATASET\n",
    "\n",
    "sql_script = f'''\n",
    "CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_MY_DATASET}.{BQ_MY_TABLE}`\n",
    "AS SELECT station_number,year,month,day,mean_temp FROM `{BQ_TABLE}`\n",
    "'''\n",
    "print(sql_script)\n",
    "\n",
    "query = bqclient.query(sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  station_number  year  month  day  mean_temp\n",
      "0          39730  1929     10   20  52.799999\n",
      "1          33110  1929     12   18  47.500000\n",
      "2          37770  1931      4   24  50.200001\n",
      "3         726810  1931      6   23  65.099998\n",
      "4         726810  1931      3    2  42.799999\n"
     ]
    }
   ],
   "source": [
    "# Download the table.\n",
    "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataset:gsod"
   },
   "source": [
    "### Read the BigQuery dataset into a tf.data.Dataset\n",
    "\n",
    "Next, you read a sample of the dataset into a tf.data.Dataset using TensorFlow IO `BigQueryClient()` and `read_session()` method, with the following parameters:\n",
    "\n",
    "- `parent`: Your project ID.\n",
    "- `project_id`: The project ID of the BigQuery table.\n",
    "- `dataset_id`: The ID of the BigQuery dataset.\n",
    "- `table_id`. The ID of the table within the corresponding BigQuery dataset.\n",
    "- `selected_fields`: Subset of fields (columns) to return.\n",
    "- `output_types`: The output types of the corresponding fields.\n",
    "- `requested_streams`: The number of parallel readers.\n",
    "\n",
    "Learn more about [BigQuery TensorFlow reader](https://www.tensorflow.org/io/tutorials/bigquery).\n",
    "\n",
    "Learn more about [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bq_to_dataset:gsod"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 01:18:37.758208: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-04-27 01:18:37.758670: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-04-27 01:18:37.870510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-04-27 01:18:37.870558: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-27 01:18:37.870580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220125-120050): /proc/driver/nvidia/version does not exist\n",
      "2022-04-27 01:18:37.870974: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: OrderedDict([(day, ()), (mean_temp, ()), (month, ()), (station_number, ()), (year, ())]), types: OrderedDict([(day, tf.int32), (mean_temp, tf.float32), (month, tf.int32), (station_number, tf.string), (year, tf.int32)])>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "\n",
    "feature_names = \"station_number,year,month,day\".split(\",\")\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "    tensorflow_io_bigquery_client = BigQueryClient()\n",
    "    read_session = tensorflow_io_bigquery_client.read_session(\n",
    "        parent=\"projects/\" + PROJECT_ID,\n",
    "        project_id=project,\n",
    "        dataset_id=dataset,\n",
    "        table_id=table,\n",
    "        selected_fields=feature_names + [target_name],\n",
    "        output_types=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    "        requested_streams=2,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "PROJECT, DATASET, TABLE = IMPORT_FILE.split(\"/\")[-1].split(\".\")\n",
    "tf_dataset = read_bigquery(PROJECT, DATASET, TABLE)\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_dataset:gsod"
   },
   "source": [
    "### Read CSV files into a tf.data.Dataset\n",
    "\n",
    "Alternatively, when your data is in CSV files, you can load the dataset into a tf.data.Dataset using `tf.data.experimental.CsvDataset`, with the following parameters:\n",
    "\n",
    "- `filenames`: A list of one or more CSV files.\n",
    "- `header`: Whether CSV file(s) contain a header.\n",
    "- `select_cols`: Subset of fields (columns) to return.\n",
    "- `record_defaults`: The output types of the corresponding fields.\n",
    "\n",
    "Learn more about [tf.data CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "csv_to_dataset:gsod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: ((), (), (), (), ()), types: (tf.string, tf.int32, tf.int32, tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "feature_names = [\"station_number,year,month,day\".split(\",\")]\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "tf_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=IMPORT_FILES,\n",
    "    header=True,\n",
    "    select_cols=feature_names.append(target_name),\n",
    "    record_defaults=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    ")\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataframe_to_bq"
   },
   "source": [
    "### Create a BigQuery dataset from a pandas dataframe\n",
    "\n",
    "You can create a BigQuery dataset from a pandas dataframe using the BigQuery `create_dataset()` and `load_table_from_dataframe()` methods, as follows:\n",
    "\n",
    "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
    " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
    "- `load_table_from_dataframe()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
    " - `dataframe`: The dataframe.\n",
    " - `table`: The `TableReference` for the table.\n",
    " - `job_config`: Specifications on how to load the dataframe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dataframe_to_bq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 rows and 5 columns to ma-mx-presales-lab.samples.gsod\n"
     ]
    }
   ],
   "source": [
    "LOCATION = \"us\"\n",
    "\n",
    "SCHEMA = [\n",
    "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "DATASET_ID = \"samples\"\n",
    "TABLE_ID = \"gsod\"\n",
    "\n",
    "def create_bigquery_dataset(dataset_id):\n",
    "    dataset = bigquery.Dataset(\n",
    "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n",
    "    )\n",
    "    dataset.location = \"us\"\n",
    "\n",
    "    try:\n",
    "        dataset = bqclient.create_dataset(dataset)  # API request\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        if err.code != 409:  # http_client.CONFLICT\n",
    "            raise\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(dataframe, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        # Specify a (partial) schema. All columns are always written to the\n",
    "        # table. The schema is used to assist in data type definitions.\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "        ],\n",
    "        # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "        # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "        # disposition it replaces the table with the loaded data.\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "    )\n",
    "\n",
    "    NEW_BQ_TABLE = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    job = bqclient.load_table_from_dataframe(\n",
    "        dataframe, NEW_BQ_TABLE, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    job.result()  # Wait for the job to complete.\n",
    "\n",
    "    table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "load_data_into_bigquery(dataframe, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_bq"
   },
   "source": [
    "### Create a BigQuery dataset from CSV files\n",
    "\n",
    "You can create a BigQuery dataset from CSV files using the BigQuery `create_dataset()` and `load_table_from_uri()` methods, as follows:\n",
    "\n",
    "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
    " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
    "- `load_table_from_uri()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
    " - `url`: A set of one or more CVS files in Cloud Storage storage.\n",
    " - `table`: The `TableReference` for the table.\n",
    " - `job_config`: Specifications on how to load the CSV data.\n",
    "\n",
    "Learn more about [Importing CSV data into BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "csv_to_bq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/ma-mx-presales-lab/datasets?prettyPrint=false: Already Exists: Dataset ma-mx-presales-lab:samples\n",
      "Starting job abfdea63-e9e0-432b-90f7-020ce81b016d\n",
      "Job finished.\n",
      "Loaded 114420316 rows.\n"
     ]
    }
   ],
   "source": [
    "LOCATION = \"us\"\n",
    "\n",
    "CSV_SCHEMA = [\n",
    "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"wban_number\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_dew_point\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_dew_point_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_sealevel_pressure\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_sealevel_pressure_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_station_pressure\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_station_pressure_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_visibility\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_visibility_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mean_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"num_mean_wind_speed_samples\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"max_sustained_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_gust_wind_speed\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_temperature\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"max_temperature_explicit\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"min_temperature\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"min_temperature_explicit\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"total_percipitation\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"snow_depth\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"fog\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"rain\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"snow\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"hail\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"thunder\", \"BOOLEAN\"),\n",
    "    bigquery.SchemaField(\"tornado\", \"BOOLEAN\"),\n",
    "]\n",
    "\n",
    "\n",
    "DATASET_ID = \"samples\"\n",
    "TABLE_ID = \"gsod\"\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(url, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.schema = CSV_SCHEMA\n",
    "    job_config.skip_leading_rows = 1  # heading\n",
    "\n",
    "    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = bqclient.get_table(table)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "\n",
    "load_data_into_bigquery(IMPORT_FILES, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_xgboost"
   },
   "source": [
    "### Read BigQuery table into XGboost DMatrix\n",
    "\n",
    "Currently, there is no direct data feeding connector between BigQuery and the open source XGBoost. The BigQuery ML service has a built-in XGBoost training module.\n",
    "\n",
    "Alernatively, you extract the data either as a pandas dataframe or as CSV files. The extracted data is then given as an input to a `DMatrix` object when training the model.\n",
    "\n",
    "Learn more about [Getting started with built-in XGBoost](https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost-start)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pandas_to_xgboost:gsod"
   },
   "source": [
    "### Read pandas table into XGboost DMatrix\n",
    "\n",
    "Next, you load the pandas dataframe into a `DMatrix` object. XGBoost does not support non-numeric inputs. Any column that is categorical need to be one-hot encoded prior to loading the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pandas_to_xgboost:gsod"
   },
   "outputs": [],
   "source": [
    "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])\n",
    "labels = dataframe[\"mean_temp\"]\n",
    "data = dataframe.drop(4)\n",
    "\n",
    "dtrain = xgb.DMatrix(data, label=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_xgboost:gsod"
   },
   "source": [
    "### Read CSV files into XGboost DMatrix\n",
    "\n",
    "Currently, there is no Cloud Storage support in XGBoost. If you use CSV files for input, you need to download them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_xgboost:gsod"
   },
   "outputs": [],
   "source": [
    "! gsutil cp $EXAMPLE_FILE data.csv\n",
    "\n",
    "dtrain = xgb.DMatrix(\"data.csv?format=csv&label_column=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Clean up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Vertex AI Dataset resource\n",
    "- Cloud Storage Bucket\n",
    "- BigQuery Dataset\n",
    "\n",
    "Set `delete_storage` to _True_ to delete the storage resources used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47ad926d84e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "# Delete the temporary BigQuery dataset\n",
    "! bq rm -r -f $PROJECT_ID:$DATASET_ID\n",
    "\n",
    "delete_storage = False\n",
    "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
    "    # Delete the created GCS bucket\n",
    "    ! gsutil rm -r $BUCKET_URI\n",
    "    # Delete the created BigQuery datasets\n",
    "    ! bq rm -r -f $PROJECT_ID:$BQ_MY_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_bq_datasets.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
