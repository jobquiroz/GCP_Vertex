{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40eace5b",
   "metadata": {},
   "source": [
    "# 05h - Vertex AI > Training > Hyperparameter Tuning Jobs With Tensorboard HPARAMS\n",
    "\n",
    "### 05 Series Overview\n",
    "Where a model gets trained is where it consumes computing resources.  With Vertex AI, you have choices for configuring the computing resources available at training.  This notebook is an example of an execution environment.  When it was set up there were choices for machine type and accelerators (GPUs).  \n",
    "\n",
    "In the `05` notebook, the model training happened directly in the notebook.  The models were then imported to Vertex AI and deployed to an endpoint for online predictions. \n",
    "\n",
    "In this `05a-05i` series of demonstrations, the same model is trained using managed computing resources in Vertex AI as custom training jobs.  These jobs will be demonstrated as:\n",
    "\n",
    "-  Custom Job from a python script (`05a`), python source distribution (`05b`), and custom container (`05c`)\n",
    "-  Training Pipeline that trains and saves models from a python script (`05d`), python source distribution (`05e`), and custom container (`05f`)\n",
    "-  Hyperparameter Tuning Jobs from a python script (`05g`), python source distribution (`05h`), and custom container (`05i`)\n",
    "\n",
    "### This Notebook (`05h`): An extension of `05b` with Hyperparmeter Tuning - And Tensorboard HParams  \n",
    "This notebook trains the same Tensorflow Keras model from 05 by first modifying and saving the training code to a python script.  Then a Python source distribution is built containing the script.  While this example fits nicely in a single script, larger examples will benefit from the flexibility offered by source distributions and this job gives an example of making the shift.  \n",
    "\n",
    "The source distribution is then used as an input for a Vertex AI Training Custom Job that is also assigned compute resources and a container (pre-built) for executing the training in a managed service.  \n",
    "\n",
    "The Custom Job is then used as the input for a Vertex AI Training Hyperparameter Tuning Job.  This runs and manages the tuning loops for the number of trials in each loop, collects the metric(s) and manages the parameters with the search algorithm for parameter modification. \n",
    "\n",
    "The training can be reviewed with Vertex AI's managed Tensorboard under Experiments > Experiments, or by clicking on the `05h...` job under Training > Hyperparameter Tuning Jobs and then clicking the 'Open Tensorboard' link.  **Click on the HParams tab in Tensorboard to review the hyperparameters and metrics.**\n",
    "\n",
    "<img src=\"architectures/overview/Training.png\">\n",
    "\n",
    "### Prerequisites:\n",
    "-  01 - BigQuery - Table Data Source\n",
    "-  Understanding:\n",
    "    -  05 - Vertex AI > Notebooks - Models Built in Notebooks with Tensorflow\n",
    "        -  Contains a more granular review of the Tensorflow model training\n",
    "\n",
    "### Overview:\n",
    "-  Setup\n",
    "-  Connect to Tensorboard instance from 05\n",
    "-  Create a `train.py` Python script that recreates the local training in 05\n",
    "-  Build a Python source distribution that contains the `train.py` script\n",
    "-  Use Python Client google.cloud.aiplatform for Vertex AI\n",
    "   -  Custom training job with aiplatform.CustomJob.from_local_script\n",
    "   -  Hyperparameter tuning job with aiplatform.HyperparameterTuningJob\n",
    "      -  Run job with .run\n",
    "   -  Upload best Model to Vertex AI with aiplatform.Model.upload\n",
    "   -  Create Endpoint with Vertex AI with aiplatform.Endpoint.create\n",
    "      -  Deploy model to endpoint with .deploy \n",
    "-  Online Prediction demonstrated using Vertex AI Endpoint with deployed model\n",
    "   -  Get records to score from BigQuery table\n",
    "   -  Prediction with aiplatform.Endpoint.predict\n",
    "   -  Prediction with REST\n",
    "   -  Prediction with gcloud (CLI)\n",
    "\n",
    "### Resources:\n",
    "-  [BigQuery Tensorflow Reader](https://www.tensorflow.org/io/tutorials/bigquery)\n",
    "-  [Keras Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "   -  [Keras API](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
    "-  [Python Client For Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n",
    "-  [Tensorflow Python Client](https://www.tensorflow.org/api_docs/python/tf)\n",
    "-  [Tensorflow I/O Python Client](https://www.tensorflow.org/io/api_docs/python/tfio/bigquery)\n",
    "-  [Python Client for Vertex AI](https://googleapis.dev/python/aiplatform/latest/aiplatform.html)\n",
    "-  [Create a Python source distribution](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container) for a Vertex AI custom training job\n",
    "-  Containers for training (Pre-Built)\n",
    "   -  [Overview](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container)\n",
    "   -  [List](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)\n",
    "-  Vertex AI Hyperparameter Tuning\n",
    "   -  [Overview of Hyperparameter Tuning](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)\n",
    "   -  [Using Hyperparameter Tuning](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning)\n",
    "-  [Tensorboard HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c02d3",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/05h_arch.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/05h_console.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6a74f",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6bc3a",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf17164",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='ma-mx-presales-lab'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = '05h'\n",
    "\n",
    "# Resources\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id' # add more variables to the string with space delimiters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f950e",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9a79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de49a4",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d98a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcf6e6",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb21c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = \"vertex-ai-mlops-bucket\"\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e5c64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'825075454589-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec6ffb",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e66c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127c117-0631-49f6-adf5-2efe7b69c880",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Get Vertex AI Experiments Tensorboard Instance Name\n",
    "[Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) has managed [Tensorboard](https://www.tensorflow.org/tensorboard) instances that you can track Tensorboard Experiments (a training run or hyperparameter tuning sweep).  \n",
    "\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID.\n",
    "\n",
    "This code checks to see if a Tensorboard Instance has been created in the project, retrieves it if so, creates it otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106cfd64-55d1-4e89-ba15-758652d06ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = aiplatform.Tensorboard.list(filter=f'display_name={DATANAME}')\n",
    "# if tb:\n",
    "#     tb = tb[0]\n",
    "# else:\n",
    "#     tb = aiplatform.Tensorboard.create(display_name = DATANAME, labels = {'notebook':f'{DATANAME}'})\n",
    "    \n",
    "# tb.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af61ff",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2080c08",
   "metadata": {},
   "source": [
    "### Assemble Python File for Training\n",
    "\n",
    "Create the main python trainer file as `/train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2952c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DIR}/source/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583e0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/05h/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from google.cloud import bigquery\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import hypertune\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--dataname', dest = 'dataname', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--notebook', dest = 'notebook', type=str)\n",
    "# hyperparameters\n",
    "parser.add_argument('--lr',dest='learning_rate', required=True, type=float, help='Learning Rate')\n",
    "parser.add_argument('--m',dest='momentum', required=True, type=float, help='Momentum')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# setup tensorboard hparams\n",
    "#    \"lr\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
    "#    \"m\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=1e-7, max=0.9, scale=\"linear\")\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate',hp.RealInterval(0.0, 1.0))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.RealInterval(0.0,1.0))\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: args.learning_rate,\n",
    "    HP_MOMENTUM: args.momentum\n",
    "}\n",
    "\n",
    "# built in parameters for data source:\n",
    "PROJECT_ID = args.project_id\n",
    "DATANAME = args.dataname\n",
    "REGION = args.region\n",
    "NOTEBOOK = args.notebook\n",
    "\n",
    "# clients\n",
    "bigquery = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {DATANAME}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{DATANAME}_prepped'\"\n",
    "schema = bigquery.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bigquery.query(query = f'SELECT DISTINCT {args.var_target} FROM {DATANAME}.{DATANAME}_prepped WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target=row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target,tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    return(row_dict, target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{PROJECT_ID}\",\n",
    "        project_id = PROJECT_ID,\n",
    "        table_id = f\"{DATANAME}_prepped\",\n",
    "        dataset_id = DATANAME,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# model input definitions\n",
    "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
    "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
    "\n",
    "# feature columns to a Dense Feature Layer\n",
    "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values())(feature_layer_inputs)\n",
    "\n",
    "# batch normalization then Dense with softmax activation to nclasses\n",
    "layers = tf.keras.layers.BatchNormalization()(feature_layer_outputs)\n",
    "layers = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax)(layers)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = feature_layer_inputs,\n",
    "    outputs = layers\n",
    ")\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = hparams[HP_LEARNING_RATE], momentum = hparams[HP_MOMENTUM]) #SGD or Adam\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(curve='PR')]\n",
    ")\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
    "hparams_callback = hp.KerasCallback(log_dir + 'train/', hparams)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback, hparams_callback], validation_data = validate)\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "\n",
    "# report hypertune info back to Vertex AI Training > Hyperparamter Tuning Job\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag = 'loss',\n",
    "    metric_value = history.history['loss'][-1],\n",
    "    global_step = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7d6b0",
   "metadata": {},
   "source": [
    "### Assemble Python Source Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80393cd",
   "metadata": {},
   "source": [
    "create `setup.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605dae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp/05h/source/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/setup.py\n",
    "from setuptools import setup\n",
    "from setuptools import find_packages\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow_io']\n",
    "\n",
    "setup(\n",
    "    name = 'trainer',\n",
    "    version = '0.1',\n",
    "    install_requires = REQUIRED_PACKAGES, \n",
    "    packages = find_packages(),\n",
    "    include_package_data = True,\n",
    "    description='Training Package'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8bb94",
   "metadata": {},
   "source": [
    "add `__init__.py` file to the trainer modules folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de1357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch {DIR}/source/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2a29a",
   "metadata": {},
   "source": [
    "Create the source distribution and copy it to the projects storage bucket:\n",
    "- change to the local direcory with the source folder\n",
    "- remove any previous distributions\n",
    "- tar and gzip the source folder\n",
    "- copy the distribution to the project folder on GCS\n",
    "- change back to the local project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ffb937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/GCP_Vertex_AI/Vertex-ai-mlops/temp/05h\n",
      "source/\n",
      "source/setup.py\n",
      "source/trainer/\n",
      "source/trainer/__init__.py\n",
      "source/trainer/train.py\n",
      "Copying file://source.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.                                      \n",
      "/home/jupyter/GCP_Vertex_AI/Vertex-ai-mlops\n"
     ]
    }
   ],
   "source": [
    "%cd {DIR}\n",
    "\n",
    "!rm -f source.tar source.tar.gz\n",
    "!tar cvf source.tar source\n",
    "!gzip source.tar\n",
    "!gsutil cp source.tar.gz {URI}/{TIMESTAMP}/source.tar.gz\n",
    "\n",
    "temp = '../'*(DIR.count('/')+1)\n",
    "%cd {temp}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a15de",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3c29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--dataname=\" + DATANAME,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--notebook=\" + NOTEBOOK\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3853000",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [f\"{URI}/{TIMESTAMP}/source.tar.gz\"],\n",
    "            \"python_module\": \"trainer.train\",\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ced17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = f\"{URI}/{TIMESTAMP}\",\n",
    "    staging_bucket = f\"{URI}/{TIMESTAMP}\",\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7ee7c",
   "metadata": {},
   "source": [
    "### Setup Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47250ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_SPEC = {\n",
    "    \"loss\": \"minimize\"\n",
    "}\n",
    "\n",
    "PARAMETER_SPEC = {\n",
    "    \"lr\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
    "    \"m\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=1e-7, max=0.9, scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e848550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "htJob = aiplatform.HyperparameterTuningJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    custom_job = customJob,\n",
    "    metric_spec = METRIC_SPEC,\n",
    "    parameter_spec = PARAMETER_SPEC,\n",
    "    max_trial_count = 20,\n",
    "    parallel_trial_count = 5,\n",
    "    search_algorithm = None,\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e25f1",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae1b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1910138120225947648?project=825075454589\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob run completed. Resource name: projects/825075454589/locations/us-central1/hyperparameterTuningJobs/1910138120225947648\n"
     ]
    }
   ],
   "source": [
    "htJob.run(\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    #tensorboard = tb.resource_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0907cf3-c896-4e94-9475-313fa53870bd",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895d2517-810c-4bf0-88ab-3248edd56540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/05h'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c492ed1-8372-4351-b3a4-52cae02f370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $DIR/all_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab7124f-9471-4c45-9ef5-2f28f60b685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/source.tar.gz...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/logs/train/events.out.tfevents.1649210960.dc54d7c4763d.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/logs/train/events.out.tfevents.1649210960.dc54d7c4763d.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/logs/validation/events.out.tfevents.1649210990.dc54d7c4763d.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/1/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/logs/train/events.out.tfevents.1649211674.367f85f4fb43.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/logs/train/events.out.tfevents.1649211675.367f85f4fb43.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/logs/validation/events.out.tfevents.1649211704.367f85f4fb43.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/10/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/logs/train/events.out.tfevents.1649212354.7b6bbdf2647e.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/logs/train/events.out.tfevents.1649212354.7b6bbdf2647e.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/logs/validation/events.out.tfevents.1649212381.7b6bbdf2647e.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/11/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/logs/train/events.out.tfevents.1649212357.a5faa81ea7b9.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/logs/train/events.out.tfevents.1649212357.a5faa81ea7b9.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/logs/validation/events.out.tfevents.1649212391.a5faa81ea7b9.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/12/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/logs/train/events.out.tfevents.1649212356.85fe755e0ddf.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/logs/train/events.out.tfevents.1649212356.85fe755e0ddf.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/logs/validation/events.out.tfevents.1649212392.85fe755e0ddf.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/13/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/logs/train/events.out.tfevents.1649212453.265f5db28dd6.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/logs/train/events.out.tfevents.1649212453.265f5db28dd6.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/logs/validation/events.out.tfevents.1649212487.265f5db28dd6.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/14/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/logs/train/events.out.tfevents.1649212483.8aed4f7deb76.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/logs/train/events.out.tfevents.1649212484.8aed4f7deb76.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/logs/validation/events.out.tfevents.1649212510.8aed4f7deb76.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/15/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/logs/train/events.out.tfevents.1649213045.3adfe038483e.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/logs/train/events.out.tfevents.1649213046.3adfe038483e.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/logs/validation/events.out.tfevents.1649213081.3adfe038483e.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/16/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/logs/train/events.out.tfevents.1649213033.ad949aa2c92f.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/logs/train/events.out.tfevents.1649213034.ad949aa2c92f.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/logs/validation/events.out.tfevents.1649213062.ad949aa2c92f.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/17/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/logs/train/events.out.tfevents.1649213129.739cddbd55db.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/logs/train/events.out.tfevents.1649213129.739cddbd55db.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/logs/validation/events.out.tfevents.1649213157.739cddbd55db.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/18/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/logs/train/events.out.tfevents.1649213160.8737bf167380.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/logs/train/events.out.tfevents.1649213161.8737bf167380.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/logs/validation/events.out.tfevents.1649213186.8737bf167380.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/19/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/logs/train/events.out.tfevents.1649210960.e2b88acd25aa.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/logs/train/events.out.tfevents.1649210960.e2b88acd25aa.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/logs/validation/events.out.tfevents.1649210987.e2b88acd25aa.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/2/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/logs/train/events.out.tfevents.1649213165.cb99dbd6127f.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/logs/train/events.out.tfevents.1649213166.cb99dbd6127f.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/logs/validation/events.out.tfevents.1649213194.cb99dbd6127f.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/20/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/logs/train/events.out.tfevents.1649210962.e83a42402a84.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/logs/train/events.out.tfevents.1649210962.e83a42402a84.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/logs/validation/events.out.tfevents.1649210992.e83a42402a84.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/3/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/logs/train/events.out.tfevents.1649210963.947d1c9b773c.380.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/logs/train/events.out.tfevents.1649210964.947d1c9b773c.380.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/logs/validation/events.out.tfevents.1649210991.947d1c9b773c.380.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/4/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/logs/train/events.out.tfevents.1649210963.e17eb729879f.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/logs/train/events.out.tfevents.1649210963.e17eb729879f.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/logs/validation/events.out.tfevents.1649210994.e17eb729879f.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/5/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/logs/train/events.out.tfevents.1649211687.d5194f278949.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/logs/train/events.out.tfevents.1649211687.d5194f278949.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/logs/validation/events.out.tfevents.1649211724.d5194f278949.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/6/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/logs/train/events.out.tfevents.1649211673.5a0e76c1419e.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/logs/train/events.out.tfevents.1649211673.5a0e76c1419e.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/logs/validation/events.out.tfevents.1649211705.5a0e76c1419e.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/7/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/logs/train/events.out.tfevents.1649211670.9eb2197ec778.379.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/logs/train/events.out.tfevents.1649211670.9eb2197ec778.379.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/logs/validation/events.out.tfevents.1649211697.9eb2197ec778.379.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/8/model/variables/variables.index...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/logs/train/events.out.tfevents.1649211674.e44b72eb78cc.378.0.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/logs/train/events.out.tfevents.1649211674.e44b72eb78cc.378.1.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/logs/validation/events.out.tfevents.1649211701.e44b72eb78cc.378.2.v2...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/model/keras_metadata.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/model/saved_model.pb...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/9/model/variables/variables.index...\n",
      "| [141 files][ 13.0 MiB/ 13.0 MiB]    1.2 MiB/s                                 \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 141 objects/13.0 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://vertex-ai-mlops-bucket/fraud/models/05h/20220406020326/* $DIR/all_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c358b3b-cb97-48c0-856b-d1cf24fd0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "706cb97d-e2dd-4085-9ef6-e97698a6cbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-33e58d38e52ac836\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-33e58d38e52ac836\");\n",
       "          const url = new URL(\"/proxy/6007/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $DIR/all_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3821d-f865-4a78-82c6-8b2d3ee74ece",
   "metadata": {},
   "source": [
    "**Web interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5828d90b-badd-415e-b72f-aefc4d3954a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 03:02:28.667670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-04-06 03:02:28.667722: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 03:02:28.667745: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220125-120050): /proc/driver/nvidia/version does not exist\n",
      "\n",
      "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/JGlT0bIzRMWt2Qd32o5BHQ/\n",
      "\n",
      "\u001b[1m[2022-04-06T03:02:28]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2022-04-06T03:02:49]\u001b[0m Total uploaded: 1800 scalars, 1240 tensors (862.3 kB), 20 binary objects (2.3 MB)\n",
      "\u001b[1m[2022-04-06T03:02:49]\u001b[0m Done scanning logdir.\n",
      "\n",
      "\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/JGlT0bIzRMWt2Qd32o5BHQ/\n"
     ]
    }
   ],
   "source": [
    "!tensorboard dev upload --logdir $DIR/all_logs \\\n",
    "  --name \"Vertex hyperparameter tunning\" \\\n",
    "  --description \"Training results from Notebook 05h: hyperparameter tunning\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab56bf6f-1cd8-43ac-ab78-e18c01b96ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 03:03:59.344981: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-04-06 03:03:59.345035: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 03:03:59.345058: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2-7-20220125-120050): /proc/driver/nvidia/version does not exist\n",
      "\n",
      "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/PNu054zlQzOPnAYFQQ2lHw/\n",
      "\n",
      "\u001b[1m[2022-04-06T03:03:59]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2022-04-06T03:04:00]\u001b[0m Total uploaded: 90 scalars, 62 tensors (43.1 kB), 1 binary objects (115.8 kB)\n",
      "\u001b[1m[2022-04-06T03:04:00]\u001b[0m Done scanning logdir.\n",
      "\n",
      "\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/PNu054zlQzOPnAYFQQ2lHw/\n"
     ]
    }
   ],
   "source": [
    "!tensorboard dev upload --logdir $DIR/all_logs/1/logs/ \\\n",
    "  --name \"Vertex hyperparameter tunning\" \\\n",
    "  --description \"Training results from Notebook 05h: hyperparameter tunning\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897c997",
   "metadata": {},
   "source": [
    "### Review the Job in Tensorboard > HParams interface:\n",
    "\n",
    "<img src=\"architectures/notebooks/05h_screenshots/hparams.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9698673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004784994293004274,\n",
       " 0.0036534483078867197,\n",
       " 0.005052285268902779,\n",
       " 0.003576806280761957,\n",
       " 0.0036575563717633486,\n",
       " 0.003697217209264636,\n",
       " 0.0042169406078755856,\n",
       " 0.00378585048019886,\n",
       " 0.003916395362466574,\n",
       " 0.003380748676136136,\n",
       " 0.0035277653951197863,\n",
       " 0.003913130145519972,\n",
       " 0.0037271794863045216,\n",
       " 0.003701823530718684,\n",
       " 0.0034728108439594507,\n",
       " 0.0034941004123538733,\n",
       " 0.003434270154684782,\n",
       " 0.003540355246514082,\n",
       " 0.003405834548175335,\n",
       " 0.0035331121180206537]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if trial.state.name == 'SUCCEEDED'\n",
    "losses = [trial.final_measurement.metrics[0].value if trial.state.name == 'SUCCEEDED' else 1 for trial in htJob.trials]\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895e08eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: \"10\"\n",
       "state: SUCCEEDED\n",
       "parameters {\n",
       "  parameter_id: \"lr\"\n",
       "  value {\n",
       "    number_value: 0.1\n",
       "  }\n",
       "}\n",
       "parameters {\n",
       "  parameter_id: \"m\"\n",
       "  value {\n",
       "    number_value: 0.28012336523404646\n",
       "  }\n",
       "}\n",
       "final_measurement {\n",
       "  step_count: 1\n",
       "  metrics {\n",
       "    metric_id: \"loss\"\n",
       "    value: 0.003380748676136136\n",
       "  }\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1647015032\n",
       "  nanos: 145598344\n",
       "}\n",
       "end_time {\n",
       "  seconds: 1647015584\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = htJob.trials[losses.index(min(losses))]\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c476d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ced262",
   "metadata": {},
   "source": [
    "### Upload The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2876b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/715288179162/locations/us-central1/models/902993715520339968/operations/4924673616865394688\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/715288179162/locations/us-central1/models/902993715520339968\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/715288179162/locations/us-central1/models/902993715520339968')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    serving_container_image_uri = DEPLOY_IMAGE,\n",
    "    artifact_uri = f\"{URI}/{TIMESTAMP}/{best.id}/model\",\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df1facf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05h_fraud_20220311155442'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8105c",
   "metadata": {},
   "source": [
    "### Create An Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a659b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/715288179162/locations/us-central1/endpoints/3878909897026306048/operations/2618830607651700736\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/715288179162/locations/us-central1/endpoints/3878909897026306048\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/715288179162/locations/us-central1/endpoints/3878909897026306048')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc7f7711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05h_fraud_20220311155442'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a8c06",
   "metadata": {},
   "source": [
    "### Deploy Model To Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663e3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying Model projects/715288179162/locations/us-central1/models/902993715520339968 to Endpoint : projects/715288179162/locations/us-central1/endpoints/3878909897026306048\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/715288179162/locations/us-central1/endpoints/3878909897026306048/operations/4370730862698823680\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/715288179162/locations/us-central1/endpoints/3878909897026306048\n"
     ]
    }
   ],
   "source": [
    "endpoint.deploy(\n",
    "    model = model,\n",
    "    deployed_model_display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    traffic_percentage = 100,\n",
    "    machine_type = DEPLOY_COMPUTE,\n",
    "    min_replica_count = 1,\n",
    "    max_replica_count = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2e12b",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50188866",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f770a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bigquery.query(query = f\"SELECT * FROM {DATANAME}.{DATANAME}_prepped WHERE splits='TEST' LIMIT 10\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f476b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7148</td>\n",
       "      <td>1.156386</td>\n",
       "      <td>0.193513</td>\n",
       "      <td>0.242220</td>\n",
       "      <td>0.660729</td>\n",
       "      <td>0.236144</td>\n",
       "      <td>0.311471</td>\n",
       "      <td>-0.088420</td>\n",
       "      <td>0.057844</td>\n",
       "      <td>1.123405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051662</td>\n",
       "      <td>-0.262183</td>\n",
       "      <td>0.477870</td>\n",
       "      <td>0.556403</td>\n",
       "      <td>-0.046953</td>\n",
       "      <td>-0.021878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0eddc3ef-a61b-4fba-a3ab-0ed9a726dcf0</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76311</td>\n",
       "      <td>-0.186529</td>\n",
       "      <td>0.545755</td>\n",
       "      <td>2.432618</td>\n",
       "      <td>3.266129</td>\n",
       "      <td>-0.784549</td>\n",
       "      <td>3.167033</td>\n",
       "      <td>-2.460489</td>\n",
       "      <td>-1.830983</td>\n",
       "      <td>0.389492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400380</td>\n",
       "      <td>-1.265280</td>\n",
       "      <td>1.231000</td>\n",
       "      <td>0.749402</td>\n",
       "      <td>0.147862</td>\n",
       "      <td>0.187856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>b1111e03-a559-4eb4-ab32-e3aea0072ef7</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125139</td>\n",
       "      <td>1.879049</td>\n",
       "      <td>0.212473</td>\n",
       "      <td>-0.085529</td>\n",
       "      <td>3.554091</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>1.188395</td>\n",
       "      <td>-0.672662</td>\n",
       "      <td>0.375249</td>\n",
       "      <td>-0.494351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131433</td>\n",
       "      <td>0.256023</td>\n",
       "      <td>-0.135450</td>\n",
       "      <td>0.048878</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0a0f4b69-01ee-436e-ae52-02237cd6433e</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51632</td>\n",
       "      <td>1.264050</td>\n",
       "      <td>0.182193</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.478060</td>\n",
       "      <td>-0.037823</td>\n",
       "      <td>-0.490973</td>\n",
       "      <td>0.166690</td>\n",
       "      <td>-0.130607</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167644</td>\n",
       "      <td>0.075563</td>\n",
       "      <td>0.698539</td>\n",
       "      <td>0.556361</td>\n",
       "      <td>-0.052595</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ed678d6e-8dea-4d45-92b7-74e7eba22402</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0    7148  1.156386  0.193513  0.242220  0.660729  0.236144  0.311471   \n",
       "1   76311 -0.186529  0.545755  2.432618  3.266129 -0.784549  3.167033   \n",
       "2  125139  1.879049  0.212473 -0.085529  3.554091  0.205505  1.188395   \n",
       "3   51632  1.264050  0.182193  0.020910  0.478060 -0.037823 -0.490973   \n",
       "\n",
       "         V7        V8        V9  ...       V23       V24       V25       V26  \\\n",
       "0 -0.088420  0.057844  1.123405  ... -0.051662 -0.262183  0.477870  0.556403   \n",
       "1 -2.460489 -1.830983  0.389492  ... -0.400380 -1.265280  1.231000  0.749402   \n",
       "2 -0.672662  0.375249 -0.494351  ...  0.131433  0.256023 -0.135450  0.048878   \n",
       "3  0.166690 -0.130607 -0.157200  ... -0.167644  0.075563  0.698539  0.556361   \n",
       "\n",
       "        V27       V28  Amount  Class                        transaction_id  \\\n",
       "0 -0.046953 -0.021878     0.0      0  0eddc3ef-a61b-4fba-a3ab-0ed9a726dcf0   \n",
       "1  0.147862  0.187856     0.0      0  b1111e03-a559-4eb4-ab32-e3aea0072ef7   \n",
       "2  0.003082 -0.042219     0.0      0  0a0f4b69-01ee-436e-ae52-02237cd6433e   \n",
       "3 -0.052595 -0.011799     0.0      0  ed678d6e-8dea-4d45-92b7-74e7eba22402   \n",
       "\n",
       "   splits  \n",
       "0    TEST  \n",
       "1    TEST  \n",
       "2    TEST  \n",
       "3    TEST  \n",
       "\n",
       "[4 rows x 33 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e9e18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "newob = pred[pred.columns[~pred.columns.isin(VAR_OMIT.split()+[VAR_TARGET, 'splits'])]].to_dict(orient='records')[0]\n",
    "#newob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5058d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [json_format.ParseDict(newob, Value())]\n",
    "parameters = json_format.ParseDict({}, Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85062554",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc643b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.999907613, 9.24144333e-05]], deployed_model_id='1870779452242264064', explanations=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances=instances, parameters=parameters)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dc3e2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999907613, 9.24144333e-05]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6fbf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef4c60",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "396dde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": [newob]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25685907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      0.999907613,\n",
      "      9.24144333e-05\n",
      "    ]\n",
      "  ],\n",
      "  \"deployedModelId\": \"1870779452242264064\",\n",
      "  \"model\": \"projects/715288179162/locations/us-central1/models/902993715520339968\",\n",
      "  \"modelDisplayName\": \"05h_fraud_20220311155442\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf26f85",
   "metadata": {},
   "source": [
    "### Get Predictions: gcloud (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ad30d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[[0.999907613, 9.24144333e-05]]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai endpoints predict {endpoint.name.rsplit('/',1)[-1]} --region={REGION} --json-request={DIR}/request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b803a30",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"99 - Cleanup\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
