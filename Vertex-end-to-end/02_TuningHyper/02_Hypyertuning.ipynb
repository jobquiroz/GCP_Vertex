{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10cf5ac-57c5-4f47-96d3-7431c32df977",
   "metadata": {},
   "source": [
    "Model with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0e0d8a-ae9f-47a3-b395-e3624124ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS]\n",
      "                             [--dropout_rate DROPOUT_RATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jupyter/.local/share/jupyter/runtime/kernel-a73cb72b-c867-41ff-af29-7aa61a15e4f8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, losses\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=5)\n",
    "parser.add_argument('--dropout_rate', dest='dropout_rate', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(args.dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aa259-c251-4e77-a94a-bfc3101413a0",
   "metadata": {},
   "source": [
    "We’ve already parameterized the dropout rate. Now we need to update the model to expose metrics. The metric we use is validation accuracy, calculated on the validation dataset at the end of each epoch. We’ll implement the metrics reporting using a custom Tensorflow callback, which calls the Hypertune Python library to report the metrics. The Hypertune library essentially just dumps the metrics in some structured format to a temporary folder on the host machine, which will be picked up by the Hypertune service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbf6cb-71e7-42db-9c7e-0d41f12c614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='val_accuracy',\n",
    "          metric_value=logs['val_accuracy'],\n",
    "          global_step=epoch\n",
    "        )\n",
    "        \n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[custom_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900de8d5-afce-4c3a-a9dd-fd7fc429a351",
   "metadata": {},
   "source": [
    "### Reestructure code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995e4ecb-a074-48ee-a9ca-8666237572d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "import argparse\n",
    "import hypertune\n",
    "\n",
    "GCS_PATH_FOR_DATA = 'gs://ma-mx-presales-lab-bucket/vertex-end-to-end/'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=5)\n",
    "parser.add_argument('--dropout_rate', dest='dropout_rate', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def extract(example):\n",
    "    data = tf.io.parse_example(\n",
    "        example,\n",
    "        # Schema of the example.\n",
    "        {\n",
    "          'image': tf.io.FixedLenFeature(shape=(32, 32, 3), dtype=tf.float32),\n",
    "          'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        }\n",
    "        )\n",
    "    return data['image'], data['label']\n",
    "\n",
    "def get_dataset(filename):\n",
    "    return tf.data.TFRecordDataset([GCS_PATH_FOR_DATA + filename]).map(extract, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(1024).batch(128).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(args.dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('train.tfrecord')\n",
    "val_dataset = get_dataset('val.tfrecord')\n",
    "test_dataset = get_dataset('test.tfrecord')\n",
    "\n",
    "\n",
    "# A distributed strategy to take advantage of available hardward.\n",
    "# No-op otherwise.\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = create_model()\n",
    "    # Restore from the latest checkpoint if available.\n",
    "    latest_ckpt = tf.train.latest_checkpoint('gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints/')\n",
    "    if latest_ckpt:\n",
    "        model.load_weights(latest_ckpt)\n",
    "\n",
    "# Create a callback to store a check at the end of each epoch.\n",
    "#ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    filepath='gs://ma-mx-presales-lab-bucket/vertex-end-to-end/checkpoints/', #+ 'val/',\n",
    "#    monitor='val_loss',\n",
    "#    save_weights_only=True\n",
    "#    )\n",
    "\n",
    "#model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[ckpt_callback])\n",
    "\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='val_accuracy',\n",
    "          metric_value=logs['val_accuracy'],\n",
    "          global_step=epoch\n",
    "        )\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[custom_callback])\n",
    "\n",
    "model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "# Export the model to GCS.\n",
    "model.save(\"gs://ma-mx-presales-lab-bucket/vertex-end-to-end/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f7a135-c4fc-49f6-8fd5-6b3b957b87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv task.py bundle/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e387857-ffad-492f-90ec-05707ef2d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python bundle/setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24507926-9b29-4261-8065-99a9dd601e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  471.0 B/  471.0 B]                                                \n",
      "Operation completed over 1 objects/471.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/trainer-0.1.tar.gz gs://ma-mx-presales-lab-bucket/vertex-end-to-end/python_code2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36236bac-532d-4b09-b764-0424fc39f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [6310487558434324480] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud beta ai hp-tuning-jobs describe 6310487558434324480 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai hp-tuning-jobs create --display-name=e2e-tutorial-hpt --region=us-central1 --config=bundle/hpt.yaml --max-trial-count=10 --parallel-trial-count=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf639dd0-e1bf-4760-aa34-65b0a9a03791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-02-17T21:52:59.717750Z'\n",
      "displayName: e2e-tutorial-hpt\n",
      "maxTrialCount: 10\n",
      "name: projects/825075454589/locations/us-central1/hyperparameterTuningJobs/6310487558434324480\n",
      "parallelTrialCount: 2\n",
      "startTime: '2022-02-17T21:53:04Z'\n",
      "state: JOB_STATE_RUNNING\n",
      "studySpec:\n",
      "  metrics:\n",
      "  - goal: MAXIMIZE\n",
      "    metricId: val_accuracy\n",
      "  parameters:\n",
      "  - doubleValueSpec:\n",
      "      maxValue: 0.9\n",
      "      minValue: 0.01\n",
      "    parameterId: dropout_rate\n",
      "trialJobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      acceleratorCount: 2\n",
      "      acceleratorType: NVIDIA_TESLA_V100\n",
      "      machineType: n1-standard-4\n",
      "    pythonPackageSpec:\n",
      "      args:\n",
      "      - --epochs=50\n",
      "      executorImageUri: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-3:latest\n",
      "      packageUris:\n",
      "      - gs://ma-mx-presales-lab-bucket/vertex-end-to-end/python_code2/trainer-0.1.tar.gz\n",
      "      pythonModule: trainer.task\n",
      "    replicaCount: '1'\n",
      "trials:\n",
      "- id: '1'\n",
      "  parameters:\n",
      "  - parameterId: dropout_rate\n",
      "    value: 0.455\n",
      "  startTime: '2022-02-17T21:53:08.983960220Z'\n",
      "  state: REQUESTED\n",
      "- id: '2'\n",
      "  parameters:\n",
      "  - parameterId: dropout_rate\n",
      "    value: 0.250313\n",
      "  startTime: '2022-02-17T21:53:08.984080477Z'\n",
      "  state: ACTIVE\n",
      "updateTime: '2022-02-17T22:01:03.760875Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai hp-tuning-jobs describe 6310487558434324480 --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6f338-ade3-41d2-90ed-dd6a3f0d633a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
